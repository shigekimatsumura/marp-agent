import subprocess
import tempfile
import base64
import os
import json
import uuid
from datetime import datetime, timedelta
from pathlib import Path

import boto3

from bedrock_agentcore import BedrockAgentCoreApp
from strands import Agent, tool
from strands.models import BedrockModel
from tavily import TavilyClient


def _get_model_config(model_type: str = "claude") -> dict:
    """ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸè¨­å®šã‚’è¿”ã™"""
    if model_type == "kimi":
        # Kimi K2 Thinkingï¼ˆMoonshot AIï¼‰
        # - ã‚¯ãƒ­ã‚¹ãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ¨è«–ãªã—
        # - cache_prompt/cache_toolséå¯¾å¿œ
        return {
            "model_id": "moonshot.kimi-k2-thinking",
            "cache_prompt": None,
            "cache_tools": None,
        }
    elif model_type == "claude5":
        # Claude Sonnet 5ï¼ˆ2026å¹´ãƒªãƒªãƒ¼ã‚¹äºˆå®šï¼‰
        # ãƒªãƒªãƒ¼ã‚¹å‰ã¯ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ãŒã€ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é€šçŸ¥
        return {
            "model_id": "us.anthropic.claude-sonnet-5-20260203-v1:0",
            "cache_prompt": "default",
            "cache_tools": "default",
        }
    else:
        # Claude Sonnet 4.5ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        return {
            "model_id": "us.anthropic.claude-sonnet-4-5-20250929-v1:0",
            "cache_prompt": "default",
            "cache_tools": "default",
        }


# Tavilyã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ï¼ˆè¤‡æ•°ã‚­ãƒ¼ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰
_tavily_clients: list[TavilyClient] = []
for _key_name in ["TAVILY_API_KEY", "TAVILY_API_KEY2", "TAVILY_API_KEY3"]:
    _key = os.environ.get(_key_name, "")
    if _key:
        _tavily_clients.append(TavilyClient(api_key=_key))


@tool
def web_search(query: str) -> str:
    """Webæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦æœ€æ–°æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆã«å¿…è¦ãªæƒ…å ±ã‚’èª¿ã¹ã‚‹éš›ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆæ—¥æœ¬èªã¾ãŸã¯è‹±èªï¼‰

    Returns:
        æ¤œç´¢çµæœã®ãƒ†ã‚­ã‚¹ãƒˆ
    """
    global _last_search_result

    if not _tavily_clients:
        return "Webæ¤œç´¢æ©Ÿèƒ½ã¯ç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆAPIã‚­ãƒ¼æœªè¨­å®šï¼‰"

    for client in _tavily_clients:
        try:
            results = client.search(
                query=query,
                max_results=5,
                search_depth="advanced",
            )
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…ã«ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            results_str = str(results).lower()
            if "usage limit" in results_str or "exceeds your plan" in results_str:
                continue  # æ¬¡ã®ã‚­ãƒ¼ã§å†è©¦è¡Œ
            # æ¤œç´¢çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆã«æ•´å½¢
            formatted_results = []
            for result in results.get("results", []):
                title = result.get("title", "")
                content = result.get("content", "")
                url = result.get("url", "")
                formatted_results.append(f"**{title}**\n{content}\nURL: {url}")
            search_result = "\n\n---\n\n".join(formatted_results) if formatted_results else "æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
            _last_search_result = search_result  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã«ä¿å­˜
            return search_result
        except Exception as e:
            error_str = str(e).lower()
            if "rate limit" in error_str or "429" in error_str or "quota" in error_str or "usage limit" in error_str:
                continue  # æ¬¡ã®ã‚­ãƒ¼ã§å†è©¦è¡Œ
            return f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}"

    # å…¨ã‚­ãƒ¼æ¯æ¸‡
    return "ç¾åœ¨ã€åˆ©ç”¨æ®ºåˆ°ã§ã¿ã®ã‚‹ã‚“ã®æ¤œç´¢APIç„¡æ–™æ ãŒæ¯æ¸‡ã—ãŸã‚ˆã†ã§ã™ã€‚ä¿®æ­£ã‚’ãŠå¾…ã¡ãã ã•ã„ğŸ™"


# ã‚¹ãƒ©ã‚¤ãƒ‰å‡ºåŠ›ç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆinvokeã§å‚ç…§ï¼‰
_generated_markdown: str | None = None

# ãƒ„ã‚¤ãƒ¼ãƒˆURLç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
_generated_tweet_url: str | None = None

# Webæ¤œç´¢çµæœç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
_last_search_result: str | None = None


@tool
def generate_tweet_url(tweet_text: str) -> str:
    """ãƒ„ã‚¤ãƒ¼ãƒˆæŠ•ç¨¿ç”¨ã®URLã‚’ç”Ÿæˆã—ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒXã§ã‚·ã‚§ã‚¢ã—ãŸã„å ´åˆã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

    Args:
        tweet_text: ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ï¼ˆ100æ–‡å­—ä»¥å†…ã€ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°å«ã‚€ï¼‰

    Returns:
        ç”Ÿæˆå®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    import urllib.parse

    global _generated_tweet_url
    # æ—¥æœ¬èªã‚’URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    encoded_text = urllib.parse.quote(tweet_text, safe='')
    # Twitter Web Intentï¼ˆcompose/postã§ã¯textãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç„¡è¦–ã•ã‚Œã‚‹ï¼‰
    _generated_tweet_url = f"https://twitter.com/intent/tweet?text={encoded_text}"
    return "ãƒ„ã‚¤ãƒ¼ãƒˆURLã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚"


@tool
def output_slide(markdown: str) -> str:
    """ç”Ÿæˆã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä½œæˆãƒ»ç·¨é›†ã—ãŸã‚‰å¿…ãšã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

    Args:
        markdown: Marpå½¢å¼ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å…¨æ–‡ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã‚’å«ã‚€ï¼‰

    Returns:
        å‡ºåŠ›å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    global _generated_markdown
    _generated_markdown = markdown
    return "ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’å‡ºåŠ›ã—ã¾ã—ãŸã€‚"

SYSTEM_PROMPT = """ã‚ãªãŸã¯ã€Œãƒ‘ãƒ¯ãƒä½œã‚‹ãƒãƒ³ã€ã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

## å½¹å‰²
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«åŸºã¥ã„ã¦ã€Marpå½¢å¼ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã§ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä½œæˆãƒ»ç·¨é›†ã—ã¾ã™ã€‚
ãƒ‡ã‚¶ã‚¤ãƒ³ã‚„æ§‹æˆã«ã¤ã„ã¦ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚‚ç©æ¥µçš„ã«è¡Œã„ã¾ã™ã€‚

## ã‚¢ãƒ—ãƒªä½¿ç”¨ã®æµã‚Œ
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã€ä½œã£ã¦ã»ã—ã„ã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒ†ãƒ¼ãƒã‚„ã€é¡Œæã®URLãªã©ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ã¾ã™ã€‚
ã‚ãªãŸã®è¿½åŠ è³ªå•ã‚„ã€ä¸€åº¦ã‚ãªãŸãŒç”Ÿæˆã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ã«å¯¾ã—ã¦ã€å†…å®¹èª¿æ•´ã‚„è»Œé“ä¿®æ­£ãªã©ã®è¿½åŠ æŒ‡ç¤ºã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ã¦ã€å£æ‰“ã¡ã—ãªãŒã‚‰ã‚¹ãƒ©ã‚¤ãƒ‰ã®å®Œæˆåº¦ã‚’é«˜ã‚ã¦ã„ãã¾ã™ã€‚

## ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆãƒ«ãƒ¼ãƒ«
- ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã‚‹ï¼š
  ---
  marp: true
  theme: gradient
  size: 16:9
  paginate: true
  ---
- ã‚¹ãƒ©ã‚¤ãƒ‰åŒºåˆ‡ã‚Šã¯ `---` ã‚’ä½¿ç”¨
- 1æšç›®ã¯ã‚¿ã‚¤ãƒˆãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰ï¼ˆã‚¿ã‚¤ãƒˆãƒ« + ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«ï¼‰
- ç®‡æ¡æ›¸ãã¯1ã‚¹ãƒ©ã‚¤ãƒ‰ã‚ãŸã‚Š3ã€œ5é …ç›®ã«æŠ‘ãˆã‚‹
- **çµµæ–‡å­—ã¯çµ¶å¯¾ã«ä½¿ç”¨ç¦æ­¢**ï¼ˆMARPã®ä»•æ§˜ã§çµµæ–‡å­—ã®å¾Œã«è‡ªå‹•æ”¹è¡ŒãŒå…¥ã‚Šã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒå´©ã‚Œã‚‹ãŸã‚ï¼‰
- **å„ã‚¹ãƒ©ã‚¤ãƒ‰ã®æœ¬æ–‡ã¯ã‚¿ã‚¤ãƒˆãƒ«ã‚’é™¤ã„ã¦8è¡Œä»¥å†…ã«åã‚ã‚‹**ï¼ˆã¯ã¿å‡ºã—é˜²æ­¢ï¼‰

## ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹æˆãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ï¼ˆå¿…ãšå¾“ã†ã“ã¨ï¼ï¼‰
å˜èª¿ãªç®‡æ¡æ›¸ãã®é€£ç¶šã‚’é¿ã‘ã€ä»¥ä¸‹ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ç¹”ã‚Šäº¤ãœã¦ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

### ã‚»ã‚¯ã‚·ãƒ§ãƒ³åŒºåˆ‡ã‚Šã‚¹ãƒ©ã‚¤ãƒ‰ã€å¿…é ˆã€‘
3ã€œ4æšã”ã¨ã«ã€èƒŒæ™¯è‰²ã‚’å¤‰ãˆãŸä¸­ã‚¿ã‚¤ãƒˆãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’æŒŸã‚“ã§ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åŒºåˆ‡ã‚‹ï¼š
```
---
<!-- _backgroundColor: #303030 -->
<!-- _color: white -->
## ã‚»ã‚¯ã‚·ãƒ§ãƒ³å
```

### å¤šæ§˜ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„å½¢å¼
ç®‡æ¡æ›¸ãã ã‘ã§ãªãã€ä»¥ä¸‹ã‚’ç©æ¥µçš„ã«ä½¿ã„åˆ†ã‘ã‚‹ï¼š
- **è¡¨ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰**: æ¯”è¼ƒãƒ»ä¸€è¦§ã«æœ€é©
- **å¼•ç”¨ãƒ–ãƒ­ãƒƒã‚¯**: é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚„å®šç¾©ã®å¼·èª¿ã« `> ãƒ†ã‚­ã‚¹ãƒˆ`
- **å¤ªå­—ãƒ»æ–œä½“**: `**é‡è¦**` ã‚„ `*è£œè¶³*`ï¼ˆ==ãƒã‚¤ãƒ©ã‚¤ãƒˆ==è¨˜æ³•ã¯æ—¥æœ¬èªã¨ç›¸æ€§ãŒæ‚ªã„ã®ã§ä½¿ç”¨ç¦æ­¢ï¼‰

### å‚è€ƒæ–‡çŒ®ãƒ»å‡ºå…¸ã‚¹ãƒ©ã‚¤ãƒ‰
Webæ¤œç´¢ã—ãŸå ´åˆã¯æœ€å¾Œã«å‡ºå…¸ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’è¿½åŠ ã—ã€æ–‡å­—ã‚’å°ã•ãã™ã‚‹ï¼š
```
---
<!-- _class: tinytext -->
## å‚è€ƒæ–‡çŒ®
- å‡ºå…¸1: ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆURLï¼‰
- å‡ºå…¸2: ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆURLï¼‰
```

### ã‚¿ã‚¤ãƒˆãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰ã®ä¾‹
```
---
<!-- _paginate: skip -->
# ãƒ—ãƒ¬ã‚¼ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
### ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ« â€” ç™ºè¡¨è€…å
```

## Webæ¤œç´¢
æœ€æ–°ã®æƒ…å ±ãŒå¿…è¦ãªå ´åˆã‚„ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ä¸æ˜ç‚¹ãŒã‚ã‚‹å ´åˆã¯ã€web_searchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦èª¿ã¹ã¦ã‹ã‚‰ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œã€‡ã€‡ã«ã¤ã„ã¦èª¿ã¹ã¦ã€ã€Œæœ€æ–°ã®ã€‡ã€‡ã€ãªã©ã¨è¨€ã£ãŸå ´åˆã¯ç©æ¥µçš„ã«æ¤œç´¢ã‚’æ´»ç”¨ã—ã¾ã™ã€‚
ä¸€åº¦ã®æ¤œç´¢ã§ååˆ†ãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œãªã‘ã‚Œã°ã€å¿…è¦ã«å¿œã˜ã¦è©¦è¡ŒéŒ¯èª¤ã—ã¦ãã ã•ã„ã€‚

## æ¤œç´¢ã‚¨ãƒ©ãƒ¼æ™‚ã®å¯¾å¿œ
web_searchãƒ„ãƒ¼ãƒ«ãŒã‚¨ãƒ©ãƒ¼ã‚’è¿”ã—ãŸå ´åˆï¼ˆã€Œæ¤œç´¢ã‚¨ãƒ©ãƒ¼ã€ã€ŒAPIã‚­ãƒ¼æœªè¨­å®šã€ã€Œrate limitã€ã€Œquotaã€ãªã©ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚€å ´åˆï¼‰ï¼š
1. ã‚¨ãƒ©ãƒ¼åŸå› ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ä¼ãˆã¦ãã ã•ã„ï¼ˆä¾‹ï¼šåˆ©ç”¨æ®ºåˆ°ã®ãŸã‚ã€ã¿ã®ã‚‹ã‚“ã®æ¤œç´¢APIç„¡æ–™æ ãŒæ¯æ¸‡ã—ãŸã‚ˆã†ã§ã™ã€‚Xã§æœ¬äººï¼ˆ@minorun365ï¼‰ã«æ•™ãˆã¦ã‚ã’ã¦ãã ã•ã„ğŸ™ï¼‰
2. ä¸€èˆ¬çš„ãªçŸ¥è­˜ã‚„æ¨æ¸¬ã§ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆã›ãšã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã€Œã¿ã®ã‚‹ã‚“ã«ã‚ˆã‚‹ä¿®æ­£ã‚’ãŠå¾…ã¡ãã ã•ã„ã€ã¨æ¡ˆå†…ã—ã¦ãã ã•ã„
3. ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆã¯è¡Œã‚ãšã€ã‚¨ãƒ©ãƒ¼å ±å‘Šã®ã¿ã§çµ‚äº†ã—ã¦ãã ã•ã„

## é‡è¦ï¼šã‚¹ãƒ©ã‚¤ãƒ‰ã®å‡ºåŠ›æ–¹æ³•
ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä½œæˆãƒ»ç·¨é›†ã—ãŸã‚‰ã€å¿…ãš output_slide ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ãƒ†ã‚­ã‚¹ãƒˆã§ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç›´æ¥æ›¸ãå‡ºã•ãªã„ã§ãã ã•ã„ã€‚output_slide ãƒ„ãƒ¼ãƒ«ã«æ¸¡ã™ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã«ã¯ã€ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã‚’å«ã‚€å®Œå…¨ãªMarpå½¢å¼ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

## ã‚¹ãƒ©ã‚¤ãƒ‰å‡ºåŠ›å¾Œã®è¿”ç­”ã«ã¤ã„ã¦
output_slide ãƒ„ãƒ¼ãƒ«ã§ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’å‡ºåŠ›ã—ãŸç›´å¾Œã¯ã€ä»¥ä¸‹ã®å ´åˆã‚’é™¤ããƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆã—ãªã„ã§ãã ã•ã„ï¼š
- Webæ¤œç´¢ãªã©ã®ãƒ„ãƒ¼ãƒ«å®Ÿè¡ŒãŒã‚¨ãƒ©ãƒ¼ã§å¤±æ•—ã—ãŸ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¿½åŠ ã§è³ªå•ã‚„ä¿®æ­£æŒ‡ç¤ºã‚’ã—ã¦ã„ã‚‹
ã€Œã‚¹ãƒ©ã‚¤ãƒ‰ãŒå®Œæˆã—ã¾ã—ãŸã€ã€Œä»¥ä¸‹ã®æ§‹æˆã§ï½ã€ãªã©ã®ã‚µãƒãƒªãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ä¸è¦ã§ã™ã€‚

## Xã§ã‚·ã‚§ã‚¢æ©Ÿèƒ½
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œã‚·ã‚§ã‚¢ã—ãŸã„ã€ã€Œãƒ„ã‚¤ãƒ¼ãƒˆã—ãŸã„ã€ã€ŒXã§å…±æœ‰ã€ãªã©ã¨è¨€ã£ãŸå ´åˆã¯ã€generate_tweet_url ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãƒ„ã‚¤ãƒ¼ãƒˆURLã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ãƒ„ã‚¤ãƒ¼ãƒˆæœ¬æ–‡ã¯ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§100æ–‡å­—ä»¥å†…ã§ä½œæˆï¼š
- #ãƒ‘ãƒ¯ãƒä½œã‚‹ãƒãƒ³ ã§â—‹â—‹ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä½œã£ã¦ã¿ã¾ã—ãŸã€‚ã“ã‚Œã¯ä¾¿åˆ©ï¼ pawapo.minoruonda.com
- â—‹â—‹ã®éƒ¨åˆ†ã¯ä½œæˆã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰ã®å†…å®¹ã‚’ç°¡æ½”ã«è¡¨ç¾

## ãã®ä»–
- ç¾åœ¨ã¯2026å¹´ã§ã™ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã€ŒPDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ããªã„ã€æ—¨ã®è³ªå•ãŒã‚ã£ãŸã‚‰ã€ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚
"""

app = BedrockAgentCoreApp()

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã”ã¨ã®Agentã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç®¡ç†ï¼ˆä¼šè©±å±¥æ­´ä¿æŒç”¨ï¼‰
_agent_sessions: dict[str, Agent] = {}


def _create_bedrock_model(model_type: str = "claude") -> BedrockModel:
    """ãƒ¢ãƒ‡ãƒ«è¨­å®šã«åŸºã¥ã„ã¦BedrockModelã‚’ä½œæˆ"""
    config = _get_model_config(model_type)
    # cache_prompt/cache_toolsãŒNoneã®å ´åˆã¯å¼•æ•°ã«å«ã‚ãªã„ï¼ˆKimi K2å¯¾å¿œï¼‰
    if config["cache_prompt"] is None:
        return BedrockModel(model_id=config["model_id"])
    else:
        return BedrockModel(
            model_id=config["model_id"],
            cache_prompt=config["cache_prompt"],
            cache_tools=config["cache_tools"],
        )


def get_or_create_agent(session_id: str | None, model_type: str = "claude") -> Agent:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã¨ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¯¾å¿œã™ã‚‹Agentã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ¼ã«ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’å«ã‚ã‚‹ï¼ˆãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆæ™‚ã«æ–°ã—ã„Agentã‚’ä½œæˆï¼‰
    cache_key = f"{session_id}:{model_type}" if session_id else None

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDãŒãªã„å ´åˆã¯æ–°è¦Agentã‚’ä½œæˆï¼ˆå±¥æ­´ãªã—ï¼‰
    if not cache_key:
        return Agent(
            model=_create_bedrock_model(model_type),
            system_prompt=SYSTEM_PROMPT,
            tools=[web_search, output_slide, generate_tweet_url],
        )

    # æ—¢å­˜ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã°ãã®Agentã‚’è¿”ã™
    if cache_key in _agent_sessions:
        return _agent_sessions[cache_key]

    # æ–°è¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å ´åˆã¯Agentã‚’ä½œæˆã—ã¦ä¿å­˜
    agent = Agent(
        model=_create_bedrock_model(model_type),
        system_prompt=SYSTEM_PROMPT,
        tools=[web_search, output_slide, generate_tweet_url],
    )
    _agent_sessions[cache_key] = agent
    return agent


# Kimi K2ã®ãƒ„ãƒ¼ãƒ«åç ´ææ¤œå‡ºç”¨
VALID_TOOL_NAMES = {"web_search", "output_slide", "generate_tweet_url"}
MAX_RETRY_COUNT = 5  # ãƒ„ãƒ¼ãƒ«åç ´ææ™‚ã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°


def is_tool_name_corrupted(tool_name: str) -> bool:
    """ãƒ„ãƒ¼ãƒ«åãŒç ´æã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆKimi K2å¯¾ç­–ï¼‰"""
    if not tool_name:
        return False
    # æœ‰åŠ¹ãªãƒ„ãƒ¼ãƒ«åã§ãªã‘ã‚Œã°ç ´æã¨ã¿ãªã™
    if tool_name not in VALID_TOOL_NAMES:
        return True
    # å†…éƒ¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ··å…¥ã—ã¦ã„ãŸã‚‰ç ´æ
    if "<|" in tool_name or "tooluse_" in tool_name:
        return True
    return False


def extract_markdown(text: str) -> str | None:
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’æŠ½å‡º"""
    import re
    pattern = r"```markdown\s*([\s\S]*?)\s*```"
    match = re.search(pattern, text)
    if match:
        return match.group(1).strip()
    return None


def remove_think_tags(text: str) -> str:
    """<think>...</think>ã‚¿ã‚°ã‚’é™¤å»ã™ã‚‹ï¼ˆKimi K2 Thinkingå¯¾ç­–ï¼‰

    Kimi K2 Thinkingãƒ¢ãƒ‡ãƒ«ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã«<think>ã‚¿ã‚°ã§æ€è€ƒéç¨‹ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚
    ã“ã‚Œã‚’ãƒãƒ£ãƒƒãƒˆæ¬„ã«è¡¨ç¤ºã—ãªã„ã‚ˆã†é™¤å»ã™ã‚‹ã€‚
    """
    import re
    # <think>...</think> ã‚¿ã‚°ã¨ãã®ä¸­èº«ã‚’é™¤å»ï¼ˆè¤‡æ•°è¡Œå¯¾å¿œï¼‰
    return re.sub(r'<think>[\s\S]*?</think>', '', text)


def extract_marp_markdown_from_text(text: str) -> str | None:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰Marpãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰

    Kimi K2ãŒoutput_slideãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã°ãšã«ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’å‡ºåŠ›ã—ãŸå ´åˆã«ä½¿ç”¨ã€‚
    ä»¥ä¸‹ã®2ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼š
    1. ç›´æ¥çš„ãªãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³: ---\nmarp: true\n...
    2. JSONå¼•æ•°å†…ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³: {"markdown": "---\\nmarp: true\\n..."}
    """
    import re
    import json

    if not text:
        return None

    # "marp: true" ã¾ãŸã¯ "marp:" ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ç‰ˆã‚‚è€ƒæ…®ï¼‰
    if "marp:" not in text and 'marp\\":' not in text:
        return None

    # ã‚±ãƒ¼ã‚¹1: JSONå¼•æ•°å†…ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’æŠ½å‡ºï¼ˆKimi K2ãŒreasoningTextå†…ã«ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’åŸ‹ã‚è¾¼ã‚“ã å ´åˆï¼‰
    # ãƒ‘ã‚¿ãƒ¼ãƒ³: <|tool_call_argument_begin|> {"markdown": "..."} <|tool_call_end|>
    json_arg_pattern = r'<\|tool_call_argument_begin\|>\s*(\{[\s\S]*?\})\s*<\|tool_call_end\|>'
    json_match = re.search(json_arg_pattern, text)
    if json_match:
        try:
            json_str = json_match.group(1)
            # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸæ”¹è¡Œã‚’å‡¦ç†
            data = json.loads(json_str)
            if isinstance(data, dict) and "markdown" in data:
                markdown = data["markdown"]
                if markdown and "marp: true" in markdown:
                    print(f"[INFO] Extracted markdown from JSON tool argument in reasoningText")
                    return markdown
        except json.JSONDecodeError as e:
            print(f"[WARN] Failed to parse JSON from tool argument: {e}")

    # ã‚±ãƒ¼ã‚¹2: ç›´æ¥çš„ãªãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’æŠ½å‡ºï¼ˆæ—¢å­˜ã®å‡¦ç†ï¼‰
    text_lower = text.lower()
    if "marp: true" in text_lower:
        # ãƒ‘ã‚¿ãƒ¼ãƒ³A: ---ã§å§‹ã¾ã‚‹ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼å½¢å¼ï¼ˆæ”¹è¡Œã¯\nã¾ãŸã¯\r\nï¼‰
        pattern_with_frontmatter = r'(---\s*[\r\n]+marp:\s*true[\s\S]*?)(?:<\|tool_call|$)'
        match = re.search(pattern_with_frontmatter, text, re.IGNORECASE)

        if not match:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³B: ---ãŒãªã„å ´åˆã€marp: trueã‹ã‚‰å§‹ã¾ã‚‹éƒ¨åˆ†ã‚’æŠ½å‡º
            # ï¼ˆKimi K2ãŒãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼è¨˜å·ãªã—ã§å‡ºåŠ›ã—ãŸå ´åˆã®å¯¾å¿œï¼‰
            pattern_without_frontmatter = r'(marp:\s*true[\s\S]*?)(?:<\|tool_call|$)'
            match = re.search(pattern_without_frontmatter, text, re.IGNORECASE)
            if match:
                # ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã®é–‹å§‹è¨˜å·ã‚’è£œå®Œ
                markdown = "---\n" + match.group(1).strip()
                print(f"[INFO] Extracted markdown without frontmatter delimiter (added ---)")
            else:
                # ãƒ‡ãƒãƒƒã‚°: ãƒãƒƒãƒã—ãªã‹ã£ãŸå ´åˆã€å…ˆé ­100æ–‡å­—ã‚’ãƒ­ã‚°å‡ºåŠ›
                preview = text[:200].replace('\n', '\\n').replace('\r', '\\r')
                print(f"[WARN] Marp markdown detected but extraction failed. Text preview: {preview}")
                return None
        else:
            markdown = match.group(1).strip()

        # å†…éƒ¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ®‹ã£ã¦ã„ãŸã‚‰é™¤å»
        markdown = re.sub(r'<\|[^>]+\|>', '', markdown)
        # æœ«å°¾ã®ä¸å®Œå…¨ãªè¡Œã‚’é™¤å»
        lines = markdown.split('\n')
        # æœ€å¾Œã®è¡ŒãŒä¸å®Œå…¨ï¼ˆé–‰ã˜ã‚¿ã‚°ãªã©ï¼‰ãªã‚‰é™¤å»
        while lines and (lines[-1].strip().startswith('<|') or not lines[-1].strip()):
            lines.pop()
        return '\n'.join(lines) if lines else None

    return None


def generate_pdf(markdown: str, theme: str = 'gradient') -> bytes:
    """Marp CLIã§PDFã‚’ç”Ÿæˆ"""
    with tempfile.TemporaryDirectory() as tmpdir:
        md_path = Path(tmpdir) / "slide.md"
        pdf_path = Path(tmpdir) / "slide.pdf"

        md_path.write_text(markdown, encoding="utf-8")

        cmd = [
            "marp",
            str(md_path),
            "--pdf",
            "--allow-local-files",
            "-o", str(pdf_path),
        ]

        # ãƒ†ãƒ¼ãƒè¨­å®š: ã‚«ã‚¹ã‚¿ãƒ CSS
        theme_path = Path(__file__).parent / f"{theme}.css"
        if theme_path.exists():
            cmd.extend(["--theme", str(theme_path)])

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            raise RuntimeError(f"Marp CLI error: {result.stderr}")

        return pdf_path.read_bytes()


def generate_pptx(markdown: str, theme: str = 'gradient') -> bytes:
    """Marp CLIã§PPTXã‚’ç”Ÿæˆ"""
    with tempfile.TemporaryDirectory() as tmpdir:
        md_path = Path(tmpdir) / "slide.md"
        pptx_path = Path(tmpdir) / "slide.pptx"

        md_path.write_text(markdown, encoding="utf-8")

        cmd = [
            "marp",
            str(md_path),
            "--pptx",
            "--allow-local-files",
            "-o", str(pptx_path),
        ]

        # ãƒ†ãƒ¼ãƒè¨­å®š: ã‚«ã‚¹ã‚¿ãƒ CSS
        theme_path = Path(__file__).parent / f"{theme}.css"
        if theme_path.exists():
            cmd.extend(["--theme", str(theme_path)])

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            raise RuntimeError(f"Marp CLI error: {result.stderr}")

        return pptx_path.read_bytes()


def generate_standalone_html(markdown: str, theme: str = 'gradient') -> str:
    """Marp CLIã§ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³HTMLã‚’ç”Ÿæˆï¼ˆå…±æœ‰ç”¨ï¼‰"""
    with tempfile.TemporaryDirectory() as tmpdir:
        md_path = Path(tmpdir) / "slide.md"
        html_path = Path(tmpdir) / "slide.html"

        md_path.write_text(markdown, encoding="utf-8")

        cmd = [
            "marp",
            str(md_path),
            "--html",
            "--allow-local-files",
            "-o", str(html_path),
        ]

        # ãƒ†ãƒ¼ãƒè¨­å®š: ã‚«ã‚¹ã‚¿ãƒ CSS
        theme_path = Path(__file__).parent / f"{theme}.css"
        if theme_path.exists():
            cmd.extend(["--theme", str(theme_path)])

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            raise RuntimeError(f"Marp CLI error: {result.stderr}")

        return html_path.read_text(encoding="utf-8")


def generate_thumbnail(markdown: str, theme: str = 'gradient') -> bytes:
    """Marp CLIã§1æšç›®ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’PNGç”»åƒã¨ã—ã¦ç”Ÿæˆï¼ˆOGPç”¨ã‚µãƒ ãƒã‚¤ãƒ«ï¼‰"""
    import re as thumb_re

    with tempfile.TemporaryDirectory() as tmpdir:
        md_path = Path(tmpdir) / "slide.md"
        png_output = Path(tmpdir) / "slide.png"

        md_path.write_text(markdown, encoding="utf-8")

        cmd = [
            "marp",
            str(md_path),
            "--image", "png",
            "--allow-local-files",
            "-o", str(png_output),
        ]

        # ãƒ†ãƒ¼ãƒè¨­å®š: ã‚«ã‚¹ã‚¿ãƒ CSS
        theme_path = Path(__file__).parent / f"{theme}.css"
        if theme_path.exists():
            cmd.extend(["--theme", str(theme_path)])

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            raise RuntimeError(f"Marp CLI thumbnail error: {result.stderr}")

        # Marpã¯è¤‡æ•°ã‚¹ãƒ©ã‚¤ãƒ‰ã®å ´åˆ slide.001.png, slide.002.png... ã‚’ç”Ÿæˆ
        # 1æšç›®ã®ã‚µãƒ ãƒã‚¤ãƒ«ã‚’å–å¾—
        png_files = sorted(Path(tmpdir).glob("slide*.png"))
        if not png_files:
            raise RuntimeError("Thumbnail generation failed: no PNG files created")

        return png_files[0].read_bytes()


def extract_slide_title(markdown: str) -> str | None:
    """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‹ã‚‰ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
    import re as title_re

    # æœ€åˆã® # è¦‹å‡ºã—ã‚’æ¢ã™
    match = title_re.search(r'^#\s+(.+)$', markdown, title_re.MULTILINE)
    if match:
        return match.group(1).strip()
    return None


def inject_ogp_tags(html: str, title: str, image_url: str, page_url: str) -> str:
    """HTMLã«OGPãƒ¡ã‚¿ã‚¿ã‚°ã‚’æŒ¿å…¥"""
    import html as html_escape

    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
    safe_title = html_escape.escape(title)

    ogp_tags = f'''
    <meta property="og:title" content="{safe_title}">
    <meta property="og:type" content="website">
    <meta property="og:url" content="{page_url}">
    <meta property="og:image" content="{image_url}">
    <meta property="og:description" content="ãƒ‘ãƒ¯ãƒä½œã‚‹ãƒãƒ³ã§ä½œæˆã—ãŸã‚¹ãƒ©ã‚¤ãƒ‰">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="{safe_title}">
    <meta name="twitter:image" content="{image_url}">
    '''
    # </head>ã®å‰ã«OGPã‚¿ã‚°ã‚’æŒ¿å…¥
    return html.replace('</head>', f'{ogp_tags}</head>')


# S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå…±æœ‰ã‚¹ãƒ©ã‚¤ãƒ‰ç”¨ï¼‰
_s3_client = None

def get_s3_client():
    """S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰"""
    global _s3_client
    if _s3_client is None:
        _s3_client = boto3.client('s3')
    return _s3_client


def share_slide(markdown: str, theme: str = 'gradient') -> dict:
    """ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’HTMLåŒ–ã—ã¦S3ã«ä¿å­˜ã—ã€å…¬é–‹URLã‚’è¿”ã™ï¼ˆOGPå¯¾å¿œï¼‰"""
    bucket_name = os.environ.get('SHARED_SLIDES_BUCKET')
    cloudfront_domain = os.environ.get('CLOUDFRONT_DOMAIN')

    if not bucket_name or not cloudfront_domain:
        raise RuntimeError("å…±æœ‰æ©Ÿèƒ½ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆç’°å¢ƒå¤‰æ•°æœªè¨­å®šï¼‰")

    # ã‚¹ãƒ©ã‚¤ãƒ‰IDç”Ÿæˆï¼ˆUUID v4ï¼‰
    slide_id = str(uuid.uuid4())
    s3_client = get_s3_client()

    # ã‚µãƒ ãƒã‚¤ãƒ«ç”Ÿæˆãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    try:
        thumbnail_bytes = generate_thumbnail(markdown, theme)
        thumbnail_key = f"slides/{slide_id}/thumbnail.png"
        s3_client.put_object(
            Bucket=bucket_name,
            Key=thumbnail_key,
            Body=thumbnail_bytes,
            ContentType='image/png',
        )
        thumbnail_url = f"https://{cloudfront_domain}/{thumbnail_key}"
        print(f"[INFO] Thumbnail uploaded: {thumbnail_url}")
    except Exception as e:
        # ã‚µãƒ ãƒã‚¤ãƒ«ç”Ÿæˆã«å¤±æ•—ã—ã¦ã‚‚HTMLå…±æœ‰ã¯ç¶šè¡Œ
        print(f"[WARN] Thumbnail generation failed: {e}")
        thumbnail_url = None

    # å…±æœ‰URLï¼ˆOGPã‚¿ã‚°æŒ¿å…¥å‰ã«æ±ºå®šï¼‰
    share_url = f"https://{cloudfront_domain}/slides/{slide_id}/index.html"

    # HTMLç”Ÿæˆ
    html_content = generate_standalone_html(markdown, theme)

    # OGPã‚¿ã‚°æŒ¿å…¥ï¼ˆã‚µãƒ ãƒã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
    if thumbnail_url:
        title = extract_slide_title(markdown) or "ã‚¹ãƒ©ã‚¤ãƒ‰"
        html_content = inject_ogp_tags(html_content, title, thumbnail_url, share_url)

    # S3ã«HTMLã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    s3_key = f"slides/{slide_id}/index.html"
    s3_client.put_object(
        Bucket=bucket_name,
        Key=s3_key,
        Body=html_content.encode('utf-8'),
        ContentType='text/html; charset=utf-8',
    )

    # æœ‰åŠ¹æœŸé™ï¼ˆ7æ—¥å¾Œï¼‰
    expires_at = int((datetime.utcnow() + timedelta(days=7)).timestamp())

    print(f"[INFO] Slide shared: {share_url} (expires: {expires_at})")

    return {
        'slideId': slide_id,
        'url': share_url,
        'expiresAt': expires_at,
    }


@app.entrypoint
async def invoke(payload, context=None):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰"""
    global _generated_markdown, _generated_tweet_url, _last_search_result
    _generated_markdown = None  # ãƒªã‚»ãƒƒãƒˆ
    _generated_tweet_url = None  # ãƒªã‚»ãƒƒãƒˆ
    _last_search_result = None  # ãƒªã‚»ãƒƒãƒˆ

    user_message = payload.get("prompt", "")
    action = payload.get("action", "chat")  # chat or export_pdf
    current_markdown = payload.get("markdown", "")
    model_type = payload.get("model_type", "claude")  # claude or kimi
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã¯HTTPãƒ˜ãƒƒãƒ€ãƒ¼çµŒç”±ã§contextã‹ã‚‰å–å¾—ï¼ˆã‚¹ãƒ†ã‚£ãƒƒã‚­ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ç”¨ï¼‰
    session_id = getattr(context, 'session_id', None) if context else None

    theme = payload.get("theme", "gradient")

    if action == "export_pdf" and current_markdown:
        # PDFå‡ºåŠ›
        try:
            pdf_bytes = generate_pdf(current_markdown, theme)
            pdf_base64 = base64.b64encode(pdf_bytes).decode("utf-8")
            yield {"type": "pdf", "data": pdf_base64}
        except Exception as e:
            yield {"type": "error", "message": str(e)}
        return

    if action == "export_pptx" and current_markdown:
        # PPTXå‡ºåŠ›
        try:
            pptx_bytes = generate_pptx(current_markdown, theme)
            pptx_base64 = base64.b64encode(pptx_bytes).decode("utf-8")
            yield {"type": "pptx", "data": pptx_base64}
        except Exception as e:
            yield {"type": "error", "message": str(e)}
        return

    if action == "share_slide" and current_markdown:
        # ã‚¹ãƒ©ã‚¤ãƒ‰å…±æœ‰ï¼ˆS3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å…¬é–‹URLã‚’è¿”ã™ï¼‰
        try:
            result = share_slide(current_markdown, theme)
            yield {
                "type": "share_result",
                "url": result['url'],
                "expiresAt": result['expiresAt'],
            }
        except Exception as e:
            yield {"type": "error", "message": str(e)}
        return

    # ç¾åœ¨ã®ã‚¹ãƒ©ã‚¤ãƒ‰ãŒã‚ã‚‹å ´åˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ä»˜åŠ 
    if current_markdown:
        user_message = f"ç¾åœ¨ã®ã‚¹ãƒ©ã‚¤ãƒ‰:\n```markdown\n{current_markdown}\n```\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤º: {user_message}"

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã¨ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¯¾å¿œã™ã‚‹Agentã‚’å–å¾—ï¼ˆä¼šè©±å±¥æ­´ãŒä¿æŒã•ã‚Œã‚‹ï¼‰
    agent = get_or_create_agent(session_id, model_type)

    # Kimi K2ã®ãƒ„ãƒ¼ãƒ«åç ´ææ™‚ã®ãƒªãƒˆãƒ©ã‚¤ãƒ«ãƒ¼ãƒ—
    retry_count = 0
    fallback_markdown: str | None = None  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³

    while retry_count <= MAX_RETRY_COUNT:
        _generated_markdown = None  # ãƒªãƒˆãƒ©ã‚¤æ™‚ã«ãƒªã‚»ãƒƒãƒˆ
        fallback_markdown = None  # ãƒªãƒˆãƒ©ã‚¤æ™‚ã«ãƒªã‚»ãƒƒãƒˆ
        tool_name_corrupted = False  # ç ´ææ¤œå‡ºãƒ•ãƒ©ã‚°
        has_any_output = False  # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒã‚ã£ãŸã‹ã®ãƒ•ãƒ©ã‚°
        web_search_executed = False  # Webæ¤œç´¢ãŒå®Ÿè¡Œã•ã‚ŒãŸã‹ã®ãƒ•ãƒ©ã‚°ï¼ˆKimi K2å¯¾ç­–ï¼‰

        # Kimi K2ã®å ´åˆã€dataã‚¤ãƒ™ãƒ³ãƒˆã‚’è“„ç©ã—ã¦ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ¤œå‡ºã«ä½¿ç”¨
        kimi_text_buffer = "" if model_type == "kimi" else None
        kimi_skip_text = False  # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ¤œå‡ºå¾Œã¯ãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—
        kimi_in_think_tag = False  # <think>ã‚¿ã‚°å†…ã‹ã©ã†ã‹ï¼ˆKimi K2 Thinkingå¯¾ç­–ï¼‰
        kimi_pending_text = ""  # <think>ã‚¿ã‚°æ¤œå‡ºç”¨ã®ãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡

        stream = agent.stream_async(user_message)

        async for event in stream:
            # Kimi K2 Thinking ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã¯ç„¡è¦–ï¼ˆæœ€çµ‚å›ç­”ã®ã¿è¡¨ç¤ºï¼‰
            if event.get("reasoning"):
                continue

            if "data" in event:
                chunk = event["data"]
                if model_type == "kimi":
                    # Kimi K2: ãƒ†ã‚­ã‚¹ãƒˆã‚’è“„ç©ã—ã¦ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ¤œå‡ºã«ä½¿ç”¨
                    kimi_text_buffer += chunk

                    # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ¤œå‡ºï¼ˆãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                    if not kimi_skip_text and "marp: true" in kimi_text_buffer.lower():
                        kimi_skip_text = True
                        print(f"[INFO] Kimi K2: Marp markdown detected in text stream, skipping text output")

                    if not kimi_skip_text:
                        # <think>ã‚¿ã‚°ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†
                        kimi_pending_text += chunk

                        # <think>ã‚¿ã‚°é–‹å§‹ã®æ¤œå‡º
                        while "<think>" in kimi_pending_text:
                            before, _, after = kimi_pending_text.partition("<think>")
                            # <think>ã‚ˆã‚Šå‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡ºåŠ›
                            if before:
                                has_any_output = True
                                yield {"type": "text", "data": before}
                            kimi_in_think_tag = True
                            kimi_pending_text = after
                            print(f"[INFO] Kimi K2: <think> tag detected, entering think mode")

                        # </think>ã‚¿ã‚°çµ‚äº†ã®æ¤œå‡º
                        while "</think>" in kimi_pending_text:
                            before, _, after = kimi_pending_text.partition("</think>")
                            kimi_in_think_tag = False
                            kimi_pending_text = after
                            print(f"[INFO] Kimi K2: </think> tag detected, exiting think mode")

                        # ã‚¿ã‚°å†…ã§ãªãã€ã‚¿ã‚°ã®æ–­ç‰‡ã§ã‚‚ãªã‘ã‚Œã°ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡ºåŠ›
                        if not kimi_in_think_tag:
                            # <think ã¾ãŸã¯ </think ã®é€”ä¸­ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æœ«å°¾ã‚’ä¿ç•™
                            safe_end = len(kimi_pending_text)
                            if "<" in kimi_pending_text:
                                last_lt = kimi_pending_text.rfind("<")
                                # æœ«å°¾7æ–‡å­—ä»¥å†…ã« < ãŒã‚ã‚Œã°ä¿ç•™ï¼ˆ<think> ã¯7æ–‡å­—ï¼‰
                                if len(kimi_pending_text) - last_lt <= 7:
                                    safe_end = last_lt
                            if safe_end > 0:
                                to_send = kimi_pending_text[:safe_end]
                                kimi_pending_text = kimi_pending_text[safe_end:]
                                if to_send:
                                    has_any_output = True
                                    yield {"type": "text", "data": to_send}
                else:
                    # Claude: ãã®ã¾ã¾ãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡
                    has_any_output = True
                    yield {"type": "text", "data": chunk}
            elif "current_tool_use" in event:
                # ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ä¸­ã‚¤ãƒ™ãƒ³ãƒˆã‚’é€ä¿¡
                tool_info = event["current_tool_use"]
                tool_name = tool_info.get("name", "unknown")
                tool_input = tool_info.get("input", {})

                # Kimi K2ã®ãƒ„ãƒ¼ãƒ«åç ´æã‚’ãƒã‚§ãƒƒã‚¯
                if is_tool_name_corrupted(tool_name):
                    tool_name_corrupted = True
                    # ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã§ã‚ã‚‹ã“ã¨ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    print(f"[WARN] Corrupted tool name detected: {tool_name[:50]}... (retry {retry_count + 1}/{MAX_RETRY_COUNT})")
                    continue  # ç ´æã—ãŸãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã¯ç„¡è¦–

                # æ–‡å­—åˆ—ã®å ´åˆã¯JSONãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã¯ä¸å®Œå…¨ãªJSONãŒæ¥ã‚‹ï¼‰
                if isinstance(tool_input, str):
                    try:
                        tool_input = json.loads(tool_input)
                    except json.JSONDecodeError:
                        pass  # ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯ãã®ã¾ã¾ï¼ˆä¸å®Œå…¨ãªJSONï¼‰

                # web_searchã®å ´åˆã¯ã‚¯ã‚¨ãƒªãŒå–å¾—ã§ããŸæ™‚ã®ã¿é€ä¿¡ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã¯è¤‡æ•°å›ã‚¤ãƒ™ãƒ³ãƒˆãŒæ¥ã‚‹ãŸã‚ï¼‰
                if tool_name == "web_search":
                    web_search_executed = True  # Webæ¤œç´¢å®Ÿè¡Œãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
                    if isinstance(tool_input, dict) and "query" in tool_input:
                        yield {"type": "tool_use", "data": tool_name, "query": tool_input["query"]}
                    # ã‚¯ã‚¨ãƒªãŒãªã„å ´åˆã¯ã‚¤ãƒ™ãƒ³ãƒˆã‚’é€ä¿¡ã—ãªã„ï¼ˆå®Œå…¨ãªJSONã‚’å¾…ã¤ï¼‰
                else:
                    yield {"type": "tool_use", "data": tool_name}
            elif "result" in event:
                # æœ€çµ‚çµæœã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆãƒ„ãƒ¼ãƒ«ä½¿ç”¨å¾Œã®å›ç­”ãªã©ï¼‰
                result = event["result"]
                if hasattr(result, 'message') and result.message:
                    for content in getattr(result.message, 'content', []):
                        # Kimi K2 Thinking ã® reasoningContent ã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                        if hasattr(content, 'reasoningContent'):
                            reasoning = content.reasoningContent
                            if hasattr(reasoning, 'reasoningText'):
                                reasoning_text = reasoning.reasoningText
                                if hasattr(reasoning_text, 'text') and reasoning_text.text:
                                    text = reasoning_text.text
                                    # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦åŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚’æ¤œå‡ºï¼ˆãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ï¼‰
                                    if "<|tool_call" in text or "functions.web_search" in text or "functions.output_slide" in text:
                                        tool_name_corrupted = True
                                        print(f"[WARN] Tool call found in reasoning text (retry {retry_count + 1}/{MAX_RETRY_COUNT})")
                                    # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
                                    extracted = extract_marp_markdown_from_text(text)
                                    if extracted and not fallback_markdown:
                                        fallback_markdown = extracted
                                        print(f"[INFO] Fallback markdown extracted from reasoningContent")
                            continue
                        if hasattr(content, 'text') and content.text:
                            has_any_output = True
                            yield {"type": "text", "data": content.text}

        # Kimi K2: ã‚¹ãƒˆãƒªãƒ¼ãƒ çµ‚äº†å¾Œã€ãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡ã®æ®‹ã‚Šã‚’å‡ºåŠ›
        if model_type == "kimi" and kimi_pending_text and not kimi_skip_text and not kimi_in_think_tag:
            # <think>ã‚¿ã‚°ã‚’é™¤å»ã—ã¦ã‹ã‚‰å‡ºåŠ›
            clean_text = remove_think_tags(kimi_pending_text)
            if clean_text:
                has_any_output = True
                yield {"type": "text", "data": clean_text}

        # Kimi K2: ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if model_type == "kimi" and kimi_text_buffer and not fallback_markdown:
            extracted = extract_marp_markdown_from_text(kimi_text_buffer)
            if extracted:
                fallback_markdown = extracted
                print(f"[INFO] Kimi K2: Fallback markdown extracted from text stream")

        # ãƒªãƒˆãƒ©ã‚¤åˆ¤å®š: ãƒ„ãƒ¼ãƒ«åç ´æãŒæ¤œå‡ºã•ã‚Œã€markdownãŒç”Ÿæˆã•ã‚Œã¦ã„ãªã„å ´åˆ
        if tool_name_corrupted and not _generated_markdown and not fallback_markdown and model_type == "kimi":
            retry_count += 1
            if retry_count <= MAX_RETRY_COUNT:
                yield {"type": "status", "data": f"ãƒªãƒˆãƒ©ã‚¤ä¸­... ({retry_count}/{MAX_RETRY_COUNT})"}
                # Agentã®ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¦ãƒªãƒˆãƒ©ã‚¤ï¼ˆç ´æã—ãŸå±¥æ­´ã‚’å¼•ãç¶™ãŒãªã„ï¼‰
                agent.messages.clear()
                continue
            else:
                yield {"type": "error", "message": "ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚Claudeãƒ¢ãƒ‡ãƒ«ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚"}
        break  # æ­£å¸¸å®Œäº†ã¾ãŸã¯ãƒªãƒˆãƒ©ã‚¤ä¸Šé™

    # output_slideãƒ„ãƒ¼ãƒ«ã§ç”Ÿæˆã•ã‚ŒãŸãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’é€ä¿¡
    # output_slideãŒå‘¼ã°ã‚Œãªã‹ã£ãŸå ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨ï¼ˆKimi K2å¯¾ç­–ï¼‰
    markdown_to_send = _generated_markdown or fallback_markdown
    if markdown_to_send:
        if fallback_markdown and not _generated_markdown:
            print(f"[INFO] Using fallback markdown (output_slide was not called)")
        yield {"type": "markdown", "data": markdown_to_send}

    # Webæ¤œç´¢å¾Œã«ã‚¹ãƒ©ã‚¤ãƒ‰ãŒç”Ÿæˆã•ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆKimi K2å¯¾ç­– #42ï¼‰
    # æ¡ä»¶: Webæ¤œç´¢ãŒå®Ÿè¡Œã•ã‚ŒãŸãŒã€ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãŒç”Ÿæˆã•ã‚Œãšã€æ¤œç´¢çµæœãŒã‚ã‚‹å ´åˆ
    if web_search_executed and not markdown_to_send and _last_search_result:
        # æ¤œç´¢çµæœã‚’500æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã¦è¡¨ç¤º
        truncated_result = _last_search_result[:500]
        if len(_last_search_result) > 500:
            truncated_result += "..."
        fallback_message = f"Webæ¤œç´¢çµæœ:\n\n{truncated_result}\n\n---\nã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä½œæˆã—ã¾ã™ã‹ï¼Ÿ"
        print(f"[INFO] Web search executed but no slide generated, returning search result as fallback (model_type={model_type})")
        yield {"type": "text", "data": fallback_message}

    # generate_tweet_urlãƒ„ãƒ¼ãƒ«ã§ç”Ÿæˆã•ã‚ŒãŸãƒ„ã‚¤ãƒ¼ãƒˆURLã‚’é€ä¿¡
    if _generated_tweet_url:
        yield {"type": "tweet_url", "data": _generated_tweet_url}

    yield {"type": "done"}


if __name__ == "__main__":
    app.run()
